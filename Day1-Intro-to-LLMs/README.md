# Day 1: Introduction to Large Language Models (LLMs)

## Key Concepts
- **LLMs** are models trained on vast amounts of text data to understand and generate human-like text.
- Two key architectures: **GPT (Generative Pre-trained Transformer)** and **BERT (Bidirectional Encoder Representations from Transformers)**.
- **Transformer** architecture uses self-attention mechanisms to efficiently process long text sequences.

## GPT vs BERT
- **GPT** is a unidirectional, decoder-only model designed for text generation.
- **BERT** is bidirectional and used for discriminative tasks like classification and question answering.

## Medium Article:
- [Read my full article on Medium](<https://medium.com/@vijayjun89/gpt-vs-bert-a-comprehensive-comparison-of-two-powerful-language-models-df27c2b45733>)